{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n# Import the refactored components\nfrom src.model_manager import ModelManager\nfrom src.evaluator import ModelEvaluator\nfrom src.visualization import plot_performance_comparison\n\n# --- CONFIGURATION ---\n# Path to the folder containing all the trained specialist models you want to compare\nMODELS_TO_COMPARE_PATH = \"/kaggle/working/output\" # Or wherever your trained models are saved\n\n# Path to a test dataset folder containing 'real' and 'fake' subdirectories\nTEST_DATA_PATH = \"/kaggle/input/your-test-dataset\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use the ModelManager to load all models from the directory\n# Note: Here we are renaming the class instance to avoid confusion\nmodel_loader = ModelManager(MODELS_TO_COMPARE_PATH)\ntrained_models = model_loader.models","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run the evaluation only if models were loaded successfully\nif trained_models:\n    # Initialize the evaluator with the loaded models and test data\n    evaluator = ModelEvaluator(\n        models_to_test=trained_models,\n        test_data_path=TEST_DATA_PATH\n    )\n    \n    # Run the comparison to get the performance metrics DataFrame\n    performance_df = evaluator.run_comparison()\n\n    # Display the results table\n    if performance_df is not None:\n        print(\"\\n--- Model Performance Leaderboard ---\")\n        display(performance_df)\nelse:\n    print(\"‚ùå No models were loaded. Cannot run evaluation.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the comparison bar chart if the DataFrame was created\nif 'performance_df' in locals() and performance_df is not None:\n    plot_performance_comparison(performance_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}